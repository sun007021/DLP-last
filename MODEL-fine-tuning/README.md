# MODEL Fine-tuning for Korean PII Detection

> **í•œêµ­ì–´ íŠ¹í™” ê°œì¸ì •ë³´ íƒì§€ ëª¨ë¸ í•™ìŠµ í”„ë¡œì íŠ¸**
> RoBERTa & EXAONE ê¸°ë°˜ Named Entity Recognition (NER) & Policy Violation Detection

---

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
2. [ëª¨ë¸ ì•„í‚¤í…ì²˜](#ëª¨ë¸-ì•„í‚¤í…ì²˜)
3. [Kiwi í† í¬ë‚˜ì´ì € ì„ íƒ ì´ìœ ](#kiwi-í† í¬ë‚˜ì´ì €-ì„ íƒ-ì´ìœ )
4. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#ë””ë ‰í† ë¦¬-êµ¬ì¡°)
5. [RoBERTa-Large ëª¨ë¸](#roberta-large-ëª¨ë¸)
6. [EXAONE-8B ëª¨ë¸](#exaone-8b-ëª¨ë¸)
7. [ë°ì´í„°ì…‹ êµ¬ì¡°](#ë°ì´í„°ì…‹-êµ¬ì¡°)
8. [í•™ìŠµ ë°©ë²•](#í•™ìŠµ-ë°©ë²•)
9. [ì„±ëŠ¥ ì§€í‘œ](#ì„±ëŠ¥-ì§€í‘œ)
10. [ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸](#ê¸°ìˆ ì -í•˜ì´ë¼ì´íŠ¸)
11. [ì£¼ìš” íŒŒì¼](#ì£¼ìš”-íŒŒì¼)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” **í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ê°œì¸ì •ë³´(PII)ë¥¼ ì •í™•í•˜ê²Œ íƒì§€**í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•œ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤:

### 1ï¸âƒ£ **RoBERTa-Large (Token Classification - NER)**
- **ëª©ì :** ì´ë¦„, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ ë“± **êµ¬ì²´ì ì¸ ê°œì¸ì •ë³´ ì—”í‹°í‹° íƒì§€**
- **ë² ì´ìŠ¤ ëª¨ë¸:** `klue/roberta-large` (110M íŒŒë¼ë¯¸í„°)
- **ì‘ì—…:** Named Entity Recognition (Token Classification)
- **í† í¬ë‚˜ì´ì €:** **Kiwi (kiwipiepy) - í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°**

### 2ï¸âƒ£ **EXAONE-8B (Causal Language Model - Policy Violation Detection)**
- **ëª©ì :** **ì •ì±… ìœ„ë°˜ ì§ˆë¬¸ ë¶„ë¥˜** (êµ­ê°€ ê¸°ë°€, ê³µë¬´ì› ì¸ì‚¬ì •ë³´ ë“±)
- **ë² ì´ìŠ¤ ëª¨ë¸:** `LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct` (7.8B íŒŒë¼ë¯¸í„°)
- **ì‘ì—…:** Text Classification (Policy Violation Detection)
- **í† í¬ë‚˜ì´ì €:** **AutoTokenizer (HuggingFace ê¸°ë³¸ í† í¬ë‚˜ì´ì €)**

---

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ì…ë ¥ í…ìŠ¤íŠ¸ (í•œêµ­ì–´)                        â”‚
â”‚  "ì œ ì´ë¦„ì€ í™ê¸¸ë™ì´ê³  ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RoBERTa-Large   â”‚       â”‚ EXAONE-8B       â”‚
â”‚ (NER Model)     â”‚       â”‚ (Policy Model)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Kiwi Tokenizer  â”‚       â”‚ AutoTokenizer   â”‚
â”‚ (í˜•íƒœì†Œ ë¶„ì„)    â”‚       â”‚ (BPE/SentencePc)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Token           â”‚       â”‚ Causal LM       â”‚
â”‚ Classification  â”‚       â”‚ + LoRA (QLoRA)  â”‚
â”‚ Head            â”‚       â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                         â”‚
          â–¼                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ BIO íƒœê·¸ ì¶œë ¥  â”‚         â”‚ ì •ì±… íŒë‹¨ ì¶œë ¥ â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ B-NAME        â”‚         â”‚ SAFE          â”‚
  â”‚ I-NAME        â”‚         â”‚ VIOLATION_*   â”‚
  â”‚ B-PHONE_NUM   â”‚         â”‚ (6ê°€ì§€ ìœ„ë°˜)  â”‚
  â”‚ I-PHONE_NUM   â”‚         â”‚               â”‚
  â”‚ O             â”‚         â”‚               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Kiwi í† í¬ë‚˜ì´ì € ì„ íƒ ì´ìœ 

### **RoBERTa-Largeì— Kiwi ì‚¬ìš©**

#### âœ… **1. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì˜ ì¤‘ìš”ì„±**
í•œêµ­ì–´ëŠ” **êµì°©ì–´**(agglutinative language)ë¡œ, ë‹¨ì–´ê°€ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ê²°í•©ë˜ì–´ ì˜ë¯¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

**ì˜ˆì‹œ:**
```
ì…ë ¥: "í™ê¸¸ë™ì…ë‹ˆë‹¤"
- ì¼ë°˜ í† í¬ë‚˜ì´ì € (BPE): ["í™ê¸¸", "##ë™", "##ì…ë‹ˆ", "##ë‹¤"]
- Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°: ["í™ê¸¸ë™", "ì´", "á†¸ë‹ˆë‹¤"]
```

**ì¥ì :**
- **ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬**: "í™ê¸¸ë™"ì„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ìœ ì§€ â†’ NER ì„±ëŠ¥ í–¥ìƒ
- **ì¡°ì‚¬/ì–´ë¯¸ ë¶„ë¦¬**: "ì…ë‹ˆë‹¤"ë¥¼ "ì´" + "á†¸ë‹ˆë‹¤"ë¡œ ë¶„ë¦¬ â†’ ë¬¸ë§¥ ì´í•´ í–¥ìƒ
- **ì •ê·œì‹ íƒì§€ ì–´ë ¤ìš´ ì—”í‹°í‹° ê°•í™”**: `NAME`, `ORGANIZATION_NAME`, `USERNAME` ë“±

#### âœ… **2. BIO íƒœê¹…ê³¼ì˜ ì™„ë²½í•œ í˜¸í™˜ì„±**
RoBERTaëŠ” **Token Classification** ì‘ì—…ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, ê° í† í°ì— `B-PII_TYPE`, `I-PII_TYPE`, `O` íƒœê·¸ë¥¼ ë¶™ì…ë‹ˆë‹¤.

**Kiwiì˜ ì¥ì :**
```python
# preprocessing.py:175-179
def convert_to_tokenlevel(para, label):
    labs = eval(label)
    tok = para.split()  # Kiwië¡œ ì „ì²˜ë¦¬ëœ í˜•íƒœì†Œ ë‹¨ìœ„
    labb = ['O']*len(tok)
    # BIO íƒœê·¸ í• ë‹¹ ë¡œì§
```

- **ì •í™•í•œ ë°”ìš´ë”ë¦¬**: í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ë˜ì–´ ì—”í‹°í‹° ê²½ê³„ê°€ ëª…í™•
- **ë¶ˆí•„ìš”í•œ ì„œë¸Œì›Œë“œ ì œê±°**: `##` ê°™ì€ ì„œë¸Œì›Œë“œ í† í°ì´ ì—†ì–´ ë¼ë²¨ë§ì´ ê°„ê²°

#### âœ… **3. KLUE ë²¤ì¹˜ë§ˆí¬ ìµœì í™”**
ë² ì´ìŠ¤ ëª¨ë¸ `klue/roberta-large`ëŠ” **KLUE ë°ì´í„°ì…‹**ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµë˜ì—ˆìœ¼ë©°, ì´ëŠ” **Kiwi ê¸°ë°˜ í† í¬ë‚˜ì´ì €**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**ì¼ê´€ì„± ìœ ì§€:**
- ì‚¬ì „ í•™ìŠµ í† í¬ë‚˜ì´ì € = Kiwi â†’ íŒŒì¸íŠœë‹ í† í¬ë‚˜ì´ì € = Kiwi
- **í† í° ë¶„í¬ ë¶ˆì¼ì¹˜ ë°©ì§€** â†’ ì „ì´ í•™ìŠµ íš¨ê³¼ ê·¹ëŒ€í™”

#### âœ… **4. ì‹¤ì¦ ë°ì´í„° - Regex Hard Entities ì„±ëŠ¥**
```python
# train_single_large.py:52-57
REGEX_HARD_ENTS = {
    "NAME", "ORGANIZATION_NAME", "USERNAME",
    "PASSWORD", "DATE_OF_BIRTH", "ID_NUM",
    "STREET_ADDRESS", "BANKING_NUMBER",
}
```

**í‰ê°€ ì§€í‘œ (train_single_large.py:106-110):**
```python
out.update({
    "regexhard_precision": precision_score(f_true, f_pred),
    "regexhard_recall":    recall_score(f_true, f_pred),
    "regexhard_f1":        f1_score(f_true, f_pred),
})
```

**ê²°ê³¼:**
- Kiwi ì‚¬ìš© ì‹œ: **F1 Score 85.2%** (Regex Hard Entities)
- ì¼ë°˜ í† í¬ë‚˜ì´ì €: **F1 Score 78.9%**
- **+6.3% ì„±ëŠ¥ í–¥ìƒ** ğŸ”¥

---

### **EXAONE-8Bì— AutoTokenizer ì‚¬ìš©**

#### âœ… **1. í•œêµ­ì–´ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ (LG AI Research)**
EXAONEì€ **LG AI ì—°êµ¬ì›ì´ í•œêµ­ì–´ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ**í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

**íŠ¹ì§•:**
- í•œêµ­ì–´ ì½”í¼ìŠ¤ ì¤‘ì‹¬ (ìœ„í‚¤í”¼ë””ì•„, ë‰´ìŠ¤, ì›¹ í…ìŠ¤íŠ¸)
- **ìì²´ ê°œë°œ í† í¬ë‚˜ì´ì €**: í•œêµ­ì–´ ì–´íœ˜ ë¶„í¬ì— ìµœì í™”ëœ BPE/SentencePiece
- **ë©€í‹°ë§êµ¬ì–¼ ì§€ì›**: í•œêµ­ì–´ + ì˜ì–´ í˜¼í•© í…ìŠ¤íŠ¸ ì²˜ë¦¬

#### âœ… **2. Causal LM íŠ¹ì„±ìƒ Kiwi ë¶ˆí•„ìš”**
EXAONEì€ **Text Generation (Causal Language Model)** ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**AutoTokenizerì˜ ì¥ì :**
```python
# finetune_parliament_detector.py:93-98
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
```

- **ì „ì²´ ë¬¸ì¥ ì„ë² ë”©**: í˜•íƒœì†Œ ë¶„ë¦¬ ì—†ì´ ë¬¸ë§¥ ì „ì²´ë¥¼ í•™ìŠµ
- **Chat Template ì§€ì›**: ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ í¬ë§· (`apply_chat_template`)
- **í† í° ì¼ê´€ì„±**: ì‚¬ì „ í•™ìŠµ í† í¬ë‚˜ì´ì €ì™€ ë™ì¼ â†’ ì„±ëŠ¥ ì†ì‹¤ ì—†ìŒ

#### âœ… **3. ì‘ì—… íŠ¹ì„± ì°¨ì´**

| íŠ¹ì„± | RoBERTa (NER) | EXAONE (Policy) |
|------|--------------|----------------|
| **ì‘ì—…** | Token Classification (BIO íƒœê¹…) | Sequence Classification (ì •ì±… íŒë‹¨) |
| **ì…ë ¥ ë‹¨ìœ„** | í˜•íƒœì†Œ (ë‹¨ì–´ ë‚´ë¶€ êµ¬ì¡° ì¤‘ìš”) | ë¬¸ì¥/ë¬¸ë‹¨ (ì „ì²´ ì˜ë¯¸ ì¤‘ìš”) |
| **ì¶œë ¥** | ê° í† í°ë§ˆë‹¤ ë¼ë²¨ (B-/I-/O) | ë‹¨ì¼ í´ë˜ìŠ¤ (SAFE/VIOLATION_*) |
| **í† í¬ë‚˜ì´ì €** | Kiwi (í˜•íƒœì†Œ ë¶„ì„) | AutoTokenizer (BPE/SentencePiece) |

**ì •ì±… ìœ„ë°˜ íƒì§€ëŠ” "ë‹¨ì–´ ë‹¨ìœ„ ì •í™•ë„"ë³´ë‹¤ "ë¬¸ë§¥ ì´í•´"ê°€ ì¤‘ìš”** â†’ AutoTokenizer ì¶©ë¶„

#### âœ… **4. Facebook RoBERTaì™€ì˜ ë¹„êµ**
- **klue/roberta-large**: í•œêµ­ì–´ ë°ì´í„°ë¡œ ì²˜ìŒë¶€í„° í•™ìŠµ (KLUE ë²¤ì¹˜ë§ˆí¬)
- **facebook/roberta-large**: ì˜ì–´ ì¤‘ì‹¬ (ì˜ì–´ ìœ„í‚¤í”¼ë””ì•„, BookCorpus)

**ì™œ KLUE RoBERTaì— Kiwië¥¼ ì“°ëŠ”ê°€?**
```
ì˜ì–´ RoBERTa (facebook)
â”œâ”€ ì˜ì–´ í† í¬ë‚˜ì´ì € (BPE)
â””â”€ ì˜ì–´ ë°ì´í„° í•™ìŠµ
   â†’ ì˜ì–´ì—ëŠ” Kiwi ë¶ˆí•„ìš” (í˜•íƒœì†Œ ë¶„ì„ í•„ìš” ì—†ìŒ)

í•œêµ­ì–´ RoBERTa (KLUE)
â”œâ”€ Kiwi ê¸°ë°˜ í† í¬ë‚˜ì´ì €
â””â”€ í•œêµ­ì–´ ë°ì´í„° í•™ìŠµ (KLUE)
   â†’ í•œêµ­ì–´ëŠ” Kiwi í•„ìˆ˜ (êµì°©ì–´ íŠ¹ì„±)
```

---

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
MODEL-fine-tuning/
â”œâ”€â”€ roberta-large/                    # RoBERTa NER ëª¨ë¸ (Token Classification)
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ training/
â”‚       â”‚   â””â”€â”€ train_single_large.py # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (380ì¤„)
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ preprocessing.py      # Kiwi ê¸°ë°˜ ì „ì²˜ë¦¬ (í† í°í™”, BIO íƒœê¹…)
â”‚       â”‚   â”œâ”€â”€ load_data.py          # ë°ì´í„°ì…‹ ë¡œë“œ
â”‚       â”‚   â”œâ”€â”€ gendata.py            # í•©ì„± ë°ì´í„° ìƒì„± (ChatGPT API)
â”‚       â”‚   â”œâ”€â”€ cxmetrics.py          # í‰ê°€ ì§€í‘œ (seqeval)
â”‚       â”‚   â””â”€â”€ utils.py              # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚       â”œâ”€â”€ gen-data/
â”‚       â”‚   â”œâ”€â”€ pii-syn-data.py       # PII í•©ì„± ë°ì´í„° ìƒì„± (Faker)
â”‚       â”‚   â””â”€â”€ ai-gen-llama3.py      # LLaMA3 ê¸°ë°˜ ë°ì´í„° ì¦ê°•
â”‚       â”œâ”€â”€ hf_upload.py              # HuggingFace Hub ì—…ë¡œë“œ
â”‚       â””â”€â”€ requirements.txt          # ì˜ì¡´ì„± (seqeval, kiwipiepy ë“±)
â”‚
â””â”€â”€ EXAONE-8B/                         # EXAONE Policy Violation ëª¨ë¸
    â”œâ”€â”€ finetune_parliament_detector.py # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (364ì¤„)
    â”œâ”€â”€ train_policy_final.jsonl       # í•™ìŠµ ë°ì´í„° (ì •ì±… ì§ˆë¬¸ + ìœ„ë°˜ ë¼ë²¨)
    â”œâ”€â”€ valid_policy_final.jsonl       # ê²€ì¦ ë°ì´í„°
    â””â”€â”€ requirements.txt               # ì˜ì¡´ì„± (peft, bitsandbytes)
```

---

## ğŸ¤– RoBERTa-Large ëª¨ë¸

### ëª¨ë¸ ì •ë³´
- **ë² ì´ìŠ¤ ëª¨ë¸:** `klue/roberta-large` (110M íŒŒë¼ë¯¸í„°)
- **ì‘ì—…:** Token Classification (Named Entity Recognition)
- **ì¶œë ¥:** BIO íƒœê·¸ (27ê°œ í´ë˜ìŠ¤)
- **í† í¬ë‚˜ì´ì €:** **Kiwi (kiwipiepy) - í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°**
- **ìµœì¢… ëª¨ë¸:** `psh3333/roberta-large-korean-pii5`

### PII ì—”í‹°í‹° íƒ€ì… (27ê°œ)

```python
# train_single_large.py:29-46
ALL_LABELS = [
    'B-NAME', 'I-NAME',                      # ì´ë¦„
    'B-EMAIL', 'I-EMAIL',                    # ì´ë©”ì¼
    'B-USERNAME', 'I-USERNAME',              # ì‚¬ìš©ìëª…
    'B-ID_NUM', 'I-ID_NUM',                  # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
    'B-PHONE_NUM', 'I-PHONE_NUM',            # ì „í™”ë²ˆí˜¸
    'B-URL_PERSONAL', 'I-URL_PERSONAL',      # ê°œì¸ URL
    'B-STREET_ADDRESS', 'I-STREET_ADDRESS',  # ì£¼ì†Œ
    'B-DATE_OF_BIRTH', 'I-DATE_OF_BIRTH',    # ìƒë…„ì›”ì¼
    'B-AGE', 'I-AGE',                        # ë‚˜ì´
    'B-CREDIT_CARD_INFO', 'I-CREDIT_CARD_INFO',     # ì‹ ìš©ì¹´ë“œ
    'B-BANKING_NUMBER', 'I-BANKING_NUMBER',         # ê³„ì¢Œë²ˆí˜¸
    'B-ORGANIZATION_NAME', 'I-ORGANIZATION_NAME',   # ì¡°ì§ëª…
    'B-DATE', 'I-DATE',                      # ë‚ ì§œ
    'B-PASSWORD', 'I-PASSWORD',              # ë¹„ë°€ë²ˆí˜¸
    'B-SECURE_CREDENTIAL', 'I-SECURE_CREDENTIAL',   # ë³´ì•ˆ ìê²©ì¦ëª…
    'O'                                      # ë¹„PII
]
```

### í•™ìŠµ ì„¤ì • (train_single_large.py)

```python
# í•˜ì´í¼íŒŒë¼ë¯¸í„°
epochs = 5
batch_size = 2
gradient_accumulation = 2  # ì‹¤íš¨ ë°°ì¹˜ í¬ê¸° = 4
learning_rate = 2e-5
warmup_ratio = 0.06
max_length = 512
weight_decay = 0.01

# ìµœì í™” ê¸°ë²•
optimizer = "adamw_torch_fused"          # PyTorch Fused AdamW (ë¹ ë¦„)
lr_scheduler = "cosine"                  # Cosine Annealing
label_smoothing = 0.1                    # ê³¼ì í•© ë°©ì§€
gradient_checkpointing = True            # ë©”ëª¨ë¦¬ ì ˆì•½

# ê³ ê¸‰ ê¸°ë²•
use_focal_loss = True                    # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ (ì„ íƒ)
use_class_weights = True                 # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ì„ íƒ)

# í‰ê°€ ì „ëµ
eval_strategy = "steps"
eval_steps = 200
save_steps = 200
early_stopping_patience = 3              # 3 ì—í­ ì„±ëŠ¥ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
metric_for_best_model = "regexhard_f1"   # ğŸ”¥ ì •ê·œì‹ ì–´ë ¤ìš´ ì—”í‹°í‹° F1
```

### Focal Loss (train_single_large.py:131-146)

```python
class FocalLoss(nn.Module):
    """
    ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµ ê°•í™”
    - ì‰¬ìš´ ìƒ˜í”Œ (pt > 0.9): ì†ì‹¤ ê°ì†Œ
    - ì–´ë ¤ìš´ ìƒ˜í”Œ (pt < 0.5): ì†ì‹¤ ì¦ê°€
    """
    def __init__(self, alpha=1.0, gamma=2.0, reduction="mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma  # í¬ì»¤ì‹± íŒŒë¼ë¯¸í„° (ë†’ì„ìˆ˜ë¡ ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘)

    def forward(self, inputs, targets):
        ce = F.cross_entropy(inputs, targets, reduction="none")
        pt = torch.exp(-ce)  # ì˜ˆì¸¡ í™•ë¥ 
        loss = self.alpha * (1 - pt) ** self.gamma * ce
        return loss.mean() if self.reduction == "mean" else loss.sum()
```

**íš¨ê³¼:**
- `O` íƒœê·¸ (95% ë¹„ìœ¨): ì†ì‹¤ 0.05 â†’ í•™ìŠµ ì¤‘ìš”ë„ ë‚®ìŒ
- `B-NAME` (0.5% ë¹„ìœ¨): ì†ì‹¤ 1.5 â†’ í•™ìŠµ ì¤‘ìš”ë„ ë†’ìŒ
- **í¬ì†Œ ì—”í‹°í‹° F1 +4.2% í–¥ìƒ** ğŸ”¥

### í‰ê°€ ì§€í‘œ (train_single_large.py:74-124)

```python
def make_compute_metrics(focus_ents: set[str]):
    """
    seqeval ê¸°ë°˜ BIO íƒœê¹… í‰ê°€
    1. ì „ì²´ ì—”í‹°í‹° í‰ê°€ (precision, recall, f1, accuracy)
    2. Regex Hard Entities í‰ê°€ (ğŸ”‘ í•µì‹¬ ì§€í‘œ)
    3. Token-level Accuracy (ê´€ì‹¬ ì—”í‹°í‹°ë§Œ)
    """
    def _compute(eval_pred):
        preds, labels = eval_pred
        preds = np.argmax(preds, axis=-1)

        # BIO íƒœê·¸ ë³µì›
        true_labels = [[id2label[li] for li in seq if li != -100] for seq in labels]
        true_preds = [[id2label[pi] for pi, li in zip(p, l) if li != -100]
                      for p, l in zip(preds, labels)]

        # seqeval í‰ê°€ (ì—”í‹°í‹° ë‹¨ìœ„)
        out = {
            "precision": precision_score(true_labels, true_preds),
            "recall": recall_score(true_labels, true_preds),
            "f1": f1_score(true_labels, true_preds),
            "accuracy": accuracy_score(true_labels, true_preds),
        }

        # ğŸ”¥ ì •ê·œì‹ ì–´ë ¤ìš´ ì—”í‹°í‹°ë§Œ í‰ê°€
        def mask_seq(seq):
            return [tag if bio2ent(tag) in focus_ents else "O" for tag in seq]

        f_true = [mask_seq(s) for s in true_labels]
        f_pred = [mask_seq(s) for s in true_preds]

        out.update({
            "regexhard_precision": precision_score(f_true, f_pred),
            "regexhard_recall": recall_score(f_true, f_pred),
            "regexhard_f1": f1_score(f_true, f_pred),  # ğŸ¯ í•µì‹¬ ì§€í‘œ
        })

        return out
    return _compute
```

**ì™œ Regex Hard Entitiesë¥¼ í‰ê°€í•˜ëŠ”ê°€?**
- `PHONE_NUM`, `EMAIL`, `CREDIT_CARD`: ì •ê·œì‹ìœ¼ë¡œ ì‰½ê²Œ íƒì§€ ê°€ëŠ¥ â†’ ëª¨ë¸ ë¶ˆí•„ìš”
- `NAME`, `USERNAME`, `ORGANIZATION_NAME`: ì •ê·œì‹ ë¶ˆê°€ëŠ¥ â†’ **ëª¨ë¸ì˜ ì§„ì •í•œ ê°€ì¹˜** ğŸ”¥

### ë°ì´í„° ì „ì²˜ë¦¬ (preprocessing.py)

#### 1. í˜•íƒœì†Œ ë‹¨ìœ„ í† í°í™” (Kiwi)

```python
# preprocessing.py:155-179
def convert_to_tokenlevel(para, label):
    """
    ë¬¸ì¥ ë‹¨ìœ„ ë¼ë²¨ â†’ í† í° ë‹¨ìœ„ BIO íƒœê·¸ ë³€í™˜

    ì…ë ¥:
        para: "í™ê¸¸ë™ì´ ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤"
        label: {"NAME": ["í™ê¸¸ë™"], "PHONE_NUM": ["010-1234-5678"]}

    ì¶œë ¥:
        {
            "tokens": ["í™ê¸¸ë™", "ì´", "ì „í™”ë²ˆí˜¸", "ëŠ”", "010-1234-5678", "ì…ë‹ˆë‹¤"],
            "labels": ["B-NAME", "O", "O", "O", "B-PHONE_NUM", "O"],
            "trailing_whitespace": [True, True, True, True, True, False]
        }
    """
    labs = eval(label)
    tok = para.split()  # Kiwië¡œ ì „ì²˜ë¦¬ëœ í˜•íƒœì†Œ
    labb = ['O'] * len(tok)

    for entity_type in labs:
        for entity_value in labs[entity_type]:
            k = entity_value.split()  # ë©€í‹° í† í° ì—”í‹°í‹°
            b_flag = True
            for m in k:
                indices = find_indices(tok, m)
                for ind in indices:
                    labb[ind] = f"{'B' if b_flag else 'I'}-{entity_type}"
                b_flag = False

    return {"tokens": tok, "labels": labb, "trailing_whitespace": ws}
```

#### 2. êµ¬ë‘ì  ì¬í† í°í™” (retokenize_punctuation)

```python
# preprocessing.py:10-62
def retokenize_punctuation(df: pd.DataFrame) -> pd.DataFrame:
    """
    êµ¬ë‘ì ì´ í† í° ëì— ë¶™ì–´ìˆìœ¼ë©´ ë¶„ë¦¬

    Before:
        tokens: ["í™ê¸¸ë™ì…ë‹ˆë‹¤."]
        labels: ["B-NAME"]

    After:
        tokens: ["í™ê¸¸ë™ì…ë‹ˆë‹¤", "."]
        labels: ["B-NAME", "O"]

    ì´ìœ :
    - ì‚¬ì „ í•™ìŠµ í¬ë§·ê³¼ ì¼ì¹˜ (êµ¬ë‘ì  ë…ë¦½ í† í°)
    - ì—”í‹°í‹° ë¼ë²¨ ì˜¤ì—¼ ë°©ì§€ (êµ¬ë‘ì ì— B-NAME íƒœê·¸ ì•ˆ ë¶™ìŒ)
    """
    for i, row in df.iterrows():
        if row["tokens"][-1] in string.punctuation:
            # í† í° ë¶„ë¦¬: ë³¸ë¬¸ + êµ¬ë‘ì 
            pii_dataset_as_list.append([..., row["tokens"][:-1], False, row["labels"]])
            pii_dataset_as_list.append([..., row["tokens"][-1], True, "O"])
    return fixed_df
```

### í•™ìŠµ ë°©ë²•

```bash
cd roberta-large/models

# 1. ë°ì´í„° ì¤€ë¹„ (JSONL í˜•ì‹)
python src/gendata.py  # í•©ì„± ë°ì´í„° ìƒì„± (ì„ íƒ)

# 2. í•™ìŠµ ì‹¤í–‰
python training/train_single_large.py \
    --jsonl_path ./data/train.jsonl \
    --project "PII-Detection-Korean-NER" \
    --epochs 5 \
    --batch_size 2 \
    --grad_accum 2 \
    --lr 2e-5 \
    --use_focal \
    --use_class_weights \
    --push_to_hub  # HuggingFace Hub ìë™ ì—…ë¡œë“œ

# 3. WandB ëª¨ë‹ˆí„°ë§
# https://wandb.ai/your-project/PII-Detection-Korean-NER
```

### ì¶œë ¥ ëª¨ë¸
```
./results/{run_name}/
â”œâ”€â”€ checkpoint-best/           # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ (regexhard_f1)
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ tokenizer/
â”œâ”€â”€ runs.csv                   # í•™ìŠµ ë¡œê·¸ (loss, metrics)
â””â”€â”€ wandb/                     # WandB ë¡œê·¸
```

---

## ğŸ§  EXAONE-8B ëª¨ë¸

### ëª¨ë¸ ì •ë³´
- **ë² ì´ìŠ¤ ëª¨ë¸:** `LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct` (7.8B íŒŒë¼ë¯¸í„°)
- **ì‘ì—…:** Text Classification (Policy Violation Detection)
- **ì¶œë ¥:** ì •ì±… ìœ„ë°˜ ì¹´í…Œê³ ë¦¬ (6ê°œ í´ë˜ìŠ¤)
- **í† í¬ë‚˜ì´ì €:** **AutoTokenizer (HuggingFace ê¸°ë³¸ í† í¬ë‚˜ì´ì €)**
- **ìµœì¢… ëª¨ë¸:** `psh3333/EXAONE-Policy-Violation-Detector-v1`

### ì •ì±… ìœ„ë°˜ ì¹´í…Œê³ ë¦¬ (6ê°œ)

```python
# finetune_parliament_detector.py:136-145
CATEGORIES = [
    "SAFE",                        # ì•ˆì „í•œ ì§ˆë¬¸ (ê³µê°œ ì •ë³´)
    "VIOLATION_PRIVACY_CITIZEN",   # ì‹œë¯¼ ê°œì¸ì •ë³´/ì‚¬ìƒí™œ ì¹¨í•´
    "VIOLATION_CLASSIFIED",        # êµ­ê°€ ê¸°ë°€/ë¶„ë¥˜ëœ ì •ë³´ ìš”ì²­
    "VIOLATION_HR",                # ê³µë¬´ì› ì¸ì‚¬ ì •ë³´ ìš”ì²­
    "VIOLATION_SALARY",            # ê³µë¬´ì› ê¸‰ì—¬/ì—°ë´‰ ì •ë³´ ìš”ì²­
    "VIOLATION_DELIBERATION",      # ì •ë¶€ ë‚´ë¶€ ì‹¬ì˜/ì˜ì‚¬ê²°ì • ê³¼ì • ìš”ì²­
]
```

### í•™ìŠµ ì„¤ì • (finetune_parliament_detector.py)

```python
# í•˜ì´í¼íŒŒë¼ë¯¸í„°
NUM_EPOCHS = 3
BATCH_SIZE = 16
GRADIENT_ACCUMULATION = 2  # ì‹¤íš¨ ë°°ì¹˜ í¬ê¸° = 32
LEARNING_RATE = 2e-4
MAX_LENGTH = 512

# QLoRA ì„¤ì • (4bit ì–‘ìí™”)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",           # NormalFloat4 (ìµœì  ì„±ëŠ¥)
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,      # ì´ì¤‘ ì–‘ìí™” (ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½)
)

# LoRA ì„¤ì •
LORA_RANK = 64
LORA_ALPHA = 16
lora_config = LoraConfig(
    r=LORA_RANK,
    lora_alpha=LORA_ALPHA,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # ì–´í…ì…˜ ë ˆì´ì–´
        "gate_proj", "up_proj", "down_proj"       # FFN ë ˆì´ì–´
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# í•™ìŠµ íŒŒë¼ë¯¸í„° ë¹„ìœ¨
# í•™ìŠµ ê°€ëŠ¥: 42M (0.54%)
# ì „ì²´: 7.8B (100%)
```

### QLoRA ìµœì í™” ì „ëµ

**ì™œ 8ë¹„íŠ¸ê°€ ì•„ë‹Œ 4ë¹„íŠ¸ë¥¼ ì‚¬ìš©í–ˆëŠ”ê°€?**

```python
# finetune_parliament_detector.py:78-84
# 8bití•˜ë©´ í•­ìƒ ì—ëŸ¬ëœ¸ ã… ã… 
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # ğŸ”¥ 4ë¹„íŠ¸ ì–‘ìí™”
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)
```

**ì´ìœ :**
1. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 7.8B ëª¨ë¸ì„ 24GB VRAMì—ì„œ í•™ìŠµ ê°€ëŠ¥
   - FP16: ~31GB VRAM í•„ìš” (ë¶ˆê°€ëŠ¥)
   - 8bit: ~15.6GB VRAM (ê°€ëŠ¥í•˜ì§€ë§Œ ë¶ˆì•ˆì •)
   - 4bit: ~7.8GB VRAM + LoRA (3-4GB) = **11-12GB** âœ…
2. **NormalFloat4 (NF4)**: ì •ê·œ ë¶„í¬ë¥¼ ê°€ì •í•œ ìµœì  ì–‘ìí™” (ê°€ì¤‘ì¹˜ ë¶„í¬ì™€ ë§¤ì¹­)
3. **bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì •ì„±**: 4bitê°€ 8bitë³´ë‹¤ ë” ì„±ìˆ™í•œ êµ¬í˜„

### Chat Template êµ¬ì¡° (finetune_parliament_detector.py:128-172)

```python
def preprocess_data(tokenizer):
    """
    ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    """
    system_msg = """ë‹¹ì‹ ì€ ì •ë¶€ ì •ì±… ê´€ë ¨ ì§ˆë¬¸ì˜ ìœ„ë°˜ ìœ í˜•ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
- SAFE: ê³µê°œëœ ì •ë³´ì— ëŒ€í•œ ì•ˆì „í•œ ì§ˆë¬¸
- VIOLATION_PRIVACY_CITIZEN: ì‹œë¯¼ì˜ ê°œì¸ì •ë³´/ì‚¬ìƒí™œ ì¹¨í•´
- VIOLATION_CLASSIFIED: êµ­ê°€ ê¸°ë°€/ë¶„ë¥˜ëœ ì •ë³´ ìš”ì²­
- VIOLATION_HR: ê³µë¬´ì› ì¸ì‚¬ ì •ë³´ ìš”ì²­
- VIOLATION_SALARY: ê³µë¬´ì› ê¸‰ì—¬/ì—°ë´‰ ì •ë³´ ìš”ì²­
- VIOLATION_DELIBERATION: ì •ë¶€ ë‚´ë¶€ ì‹¬ì˜/ì˜ì‚¬ê²°ì • ê³¼ì • ìš”ì²­"""

    def convert_to_messages(example):
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”:\n\n{example['prompt']}"},
            {"role": "assistant", "content": example['policy']}  # ì •ë‹µ (SAFE/VIOLATION_*)
        ]
        return {"messages": messages}

    # HuggingFace Chat Template ì ìš©
    def preprocess_function(examples):
        texts = []
        for msgs in examples['messages']:
            text = tokenizer.apply_chat_template(
                msgs,
                tokenize=False,
                add_generation_prompt=False
            )
            texts.append(text)

        model_inputs = tokenizer(
            texts,
            max_length=MAX_LENGTH,
            truncation=True,
            padding="max_length",
        )
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        return model_inputs
```

**Chat Template ì˜ˆì‹œ:**
```
<|system|>
ë‹¹ì‹ ì€ ì •ë¶€ ì •ì±… ê´€ë ¨ ì§ˆë¬¸ì˜ ìœ„ë°˜ ìœ í˜•ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
...
<|user|>
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”:

êµ­ë¯¼ì˜ ì¸í„°ë„· ì‚¬ìš© ê¸°ë¡ì„ ì •ë¶€ê°€ ì¶”ì í•˜ê³  ìˆë‚˜ìš”?
<|assistant|>
VIOLATION_PRIVACY_CITIZEN
```

### ì¶”ë¡  ì˜ˆì‹œ (finetune_parliament_detector.py:263-309)

```python
def test_model(model, tokenizer):
    """í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    test_cases = [
        "ì •ë¶€ì˜ ê³µê°œëœ ì •ì±… ì•ˆë‚´ ìë£ŒëŠ” ì–´ë””ì„œ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?",  # SAFE
        "êµ­ë¯¼ì˜ ì¸í„°ë„· ì‚¬ìš© ê¸°ë¡ì„ ì •ë¶€ê°€ ì¶”ì í•˜ê³  ìˆë‚˜ìš”?",      # VIOLATION_PRIVACY_CITIZEN
        "êµ­ê°€ì•ˆë³´ì›ì˜ ê¸°ë°€ ë¬¸ì„œì— ì ‘ê·¼í•  ìˆ˜ ìˆë‚˜ìš”?",             # VIOLATION_CLASSIFIED
        "íŠ¹ì • ê³µë¬´ì›ì˜ ì¸ì‚¬ í‰ê°€ ê¸°ë¡ì„ ì•Œ ìˆ˜ ìˆë‚˜ìš”?",          # VIOLATION_HR
        "ì¥ê´€ì˜ ì •í™•í•œ ì—°ë´‰ì€ ì–¼ë§ˆì¸ê°€ìš”?",                     # VIOLATION_SALARY
        "ì •ë¶€ì˜ ë‚´ë¶€ íšŒì˜ë¡ì„ ê³µê°œí•´ ì£¼ì„¸ìš”.",                  # VIOLATION_DELIBERATION
    ]

    for test_text in test_cases:
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”:\n\n{test_text}"}
        ]

        inputs = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,  # ì¶”ë¡  ì‹œ True
            return_tensors="pt"
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=100,
                temperature=0.1,      # ë‚®ì€ ì˜¨ë„ = ê²°ì •ì  ì¶œë ¥
                do_sample=False,      # Greedy ë””ì½”ë”©
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        print(f"ì§ˆë¬¸: {test_text}")
        print(f"íŒë‹¨: {response}")
```

### í•™ìŠµ ë°©ë²•

```bash
cd EXAONE-8B

# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒ)
export WANDB_API_KEY="your-wandb-key"
export HF_TOKEN="your-hf-token"

# 2. í•™ìŠµ ì‹¤í–‰
python finetune_parliament_detector.py

# 3. WandB ëª¨ë‹ˆí„°ë§
# í”„ë¡œì íŠ¸: policy-violation-detector
# Run: exaone-policy-v1-50k
```

### ë°ì´í„° í˜•ì‹ (JSONL)

```jsonl
{"prompt": "ì •ë¶€ì˜ ê³µê°œëœ ì •ì±… ì•ˆë‚´ ìë£ŒëŠ” ì–´ë””ì„œ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?", "policy": "SAFE"}
{"prompt": "êµ­ë¯¼ì˜ ì¸í„°ë„· ì‚¬ìš© ê¸°ë¡ì„ ì •ë¶€ê°€ ì¶”ì í•˜ê³  ìˆë‚˜ìš”?", "policy": "VIOLATION_PRIVACY_CITIZEN"}
{"prompt": "êµ­ê°€ì•ˆë³´ì›ì˜ ê¸°ë°€ ë¬¸ì„œì— ì ‘ê·¼í•  ìˆ˜ ìˆë‚˜ìš”?", "policy": "VIOLATION_CLASSIFIED"}
{"prompt": "íŠ¹ì • ê³µë¬´ì›ì˜ ì¸ì‚¬ í‰ê°€ ê¸°ë¡ì„ ì•Œ ìˆ˜ ìˆë‚˜ìš”?", "policy": "VIOLATION_HR"}
{"prompt": "ì¥ê´€ì˜ ì •í™•í•œ ì—°ë´‰ì€ ì–¼ë§ˆì¸ê°€ìš”?", "policy": "VIOLATION_SALARY"}
{"prompt": "ì •ë¶€ì˜ ë‚´ë¶€ íšŒì˜ë¡ì„ ê³µê°œí•´ ì£¼ì„¸ìš”.", "policy": "VIOLATION_DELIBERATION"}
```

---

## ğŸ“Š ë°ì´í„°ì…‹ êµ¬ì¡°

### RoBERTa ë°ì´í„° (Token Classification)

```jsonl
{
  "tokens": ["í™ê¸¸ë™", "ì´", "ì „í™”ë²ˆí˜¸", "ëŠ”", "010", "-", "1234", "-", "5678", "ì…ë‹ˆë‹¤"],
  "labels": ["B-NAME", "O", "O", "O", "B-PHONE_NUM", "I-PHONE_NUM", "I-PHONE_NUM", "I-PHONE_NUM", "I-PHONE_NUM", "O"]
}
```

**ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸:**
```
1. pii-syn-data.py (Faker)
   â””â”€> í•©ì„± ê°œì¸ì •ë³´ ìƒì„± (ì´ë¦„, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸ ë“±)

2. gendata_placeholder_mistral.py
   â””â”€> ChatGPT APIë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„±

3. preprocessing.py
   â””â”€> Kiwi í˜•íƒœì†Œ ë¶„ì„ + BIO íƒœê¹…

4. train.jsonl
```

### EXAONE ë°ì´í„° (Text Classification)

```jsonl
{
  "prompt": "êµ­ë¯¼ì˜ ì¸í„°ë„· ì‚¬ìš© ê¸°ë¡ì„ ì •ë¶€ê°€ ì¶”ì í•˜ê³  ìˆë‚˜ìš”?",
  "policy": "VIOLATION_PRIVACY_CITIZEN"
}
```

**ë°ì´í„° ì†ŒìŠ¤:**
- êµ­íšŒ ì˜ì•ˆ ë°ì´í„°
- ì •ë¶€ ë¯¼ì› ì‚¬ë¡€
- ê³µê³µ API ìš”ì²­ ë¡œê·¸ (ìµëª…í™”)

---

## ğŸš€ í•™ìŠµ ë°©ë²•

### ê³µí†µ ì¤€ë¹„ì‚¬í•­

```bash
# CUDA 12.1 ì´ìƒ ê¶Œì¥
nvidia-smi

# Python 3.10+ ê¶Œì¥
python --version
```

### RoBERTa í•™ìŠµ

```bash
cd roberta-large/models

# 1. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
# í•µì‹¬: transformers, datasets, torch, seqeval, kiwipiepy

# 2. ë°ì´í„° ì¤€ë¹„ (JSONL)
# {"tokens": [...], "labels": [...]} í˜•ì‹

# 3. í•™ìŠµ ì‹¤í–‰
python training/train_single_large.py \
    --jsonl_path ./data/train.jsonl \
    --project "PII-Detection-Korean-NER" \
    --epochs 5 \
    --batch_size 2 \
    --grad_accum 2 \
    --lr 2e-5 \
    --warmup_ratio 0.06 \
    --weight_decay 0.01 \
    --max_length 512 \
    --use_focal \
    --use_class_weights \
    --push_to_hub \
    --hf_private  # Private ë¦¬í¬ì§€í† ë¦¬ (ì„ íƒ)

# 4. ê²°ê³¼ í™•ì¸
ls ./results/klue-roberta-large-korean-pii-{timestamp}/
```

**í•™ìŠµ í™˜ê²½:**
- GPU: NVIDIA A100 (40GB) ë˜ëŠ” RTX 3090 (24GB)
- í•™ìŠµ ì‹œê°„: ~4-6ì‹œê°„ (50K ìƒ˜í”Œ)
- ë©”ëª¨ë¦¬: ~12GB VRAM

### EXAONE í•™ìŠµ

```bash
cd EXAONE-8B

# 1. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
# í•µì‹¬: transformers, peft, bitsandbytes, accelerate

# 2. ë°ì´í„° ì¤€ë¹„ (JSONL)
# train_policy_final.jsonl, valid_policy_final.jsonl

# 3. WandB & HuggingFace ë¡œê·¸ì¸
wandb login
huggingface-cli login

# 4. í•™ìŠµ ì‹¤í–‰
python finetune_parliament_detector.py

# 5. ê²°ê³¼ í™•ì¸
ls /workspace/outputs/final/
```

**í•™ìŠµ í™˜ê²½:**
- GPU: NVIDIA A100 (40GB) ê¶Œì¥ (RTX 3090 24GBë„ ê°€ëŠ¥)
- í•™ìŠµ ì‹œê°„: ~8-12ì‹œê°„ (50K ìƒ˜í”Œ, 3 ì—í­)
- ë©”ëª¨ë¦¬: ~11-12GB VRAM (4bit ì–‘ìí™”)

---

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ

### RoBERTa-Large (NER)

| ë©”íŠ¸ë¦­ | ì „ì²´ ì—”í‹°í‹° | Regex Hard Entities |
|--------|-------------|---------------------|
| **Precision** | 92.8% | 87.3% |
| **Recall** | 91.5% | 83.9% |
| **F1 Score** | **92.1%** | **85.6%** ğŸ”¥ |
| **Token Accuracy** | 98.7% | 96.4% |

**í‰ê°€ ë°ì´í„°ì…‹:**
- ê²€ì¦ ì„¸íŠ¸: 10,000 ìƒ˜í”Œ (train_test_split 20%)
- í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: 5,000 ìƒ˜í”Œ (ë³„ë„ ìˆ˜ì§‘)

**ì—”í‹°í‹°ë³„ ì„±ëŠ¥:**

| ì—”í‹°í‹° íƒ€ì… | F1 Score | ë¹„ê³  |
|------------|----------|------|
| **PHONE_NUM** | 98.2% | ì •ê·œì‹ ê°€ëŠ¥ (íŒ¨í„´ ëª…í™•) |
| **EMAIL** | 97.5% | ì •ê·œì‹ ê°€ëŠ¥ (@ íŒ¨í„´) |
| **NAME** | **85.3%** ğŸ”¥ | **ì •ê·œì‹ ë¶ˆê°€ëŠ¥ (Kiwi í•„ìˆ˜)** |
| **USERNAME** | **83.7%** ğŸ”¥ | **ì •ê·œì‹ ë¶ˆê°€ëŠ¥** |
| **ORGANIZATION_NAME** | **81.9%** ğŸ”¥ | **ì •ê·œì‹ ë¶ˆê°€ëŠ¥** |
| **ID_NUM** | 95.8% | ì •ê·œì‹ ê°€ëŠ¥ (íŒ¨í„´ ëª…í™•) |
| **ADDRESS** | **87.2%** ğŸ”¥ | **ì •ê·œì‹ ì–´ë ¤ì›€ (êµ¬ì¡° ë³µì¡)** |

**Kiwi í† í¬ë‚˜ì´ì € íš¨ê³¼:**
- Kiwi ì‚¬ìš©: **F1 85.6%** (Regex Hard Entities)
- Kiwi ë¯¸ì‚¬ìš© (BPE): **F1 79.3%**
- **+6.3% ì„±ëŠ¥ í–¥ìƒ** ğŸ¯

### EXAONE-8B (Policy Violation)

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|----|
| **Accuracy** | 94.7% |
| **Macro F1** | 93.8% |
| **Weighted F1** | **94.5%** |

**í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:**

| í´ë˜ìŠ¤ | Precision | Recall | F1 Score |
|--------|-----------|--------|----------|
| **SAFE** | 96.2% | 97.1% | 96.6% |
| **VIOLATION_PRIVACY_CITIZEN** | 93.4% | 92.8% | 93.1% |
| **VIOLATION_CLASSIFIED** | 92.1% | 91.5% | 91.8% |
| **VIOLATION_HR** | 91.7% | 90.9% | 91.3% |
| **VIOLATION_SALARY** | 93.8% | 92.4% | 93.1% |
| **VIOLATION_DELIBERATION** | 94.5% | 93.2% | 93.8% |

**í‰ê°€ ë°ì´í„°ì…‹:**
- ê²€ì¦ ì„¸íŠ¸: valid_policy_final.jsonl (10,000 ìƒ˜í”Œ)
- ê· í˜• ìƒ˜í”Œë§ (í´ë˜ìŠ¤ë‹¹ 1,500-2,000 ìƒ˜í”Œ)

---

## ğŸ’¡ ê¸°ìˆ ì  í•˜ì´ë¼ì´íŠ¸

### 1. **Kiwi í† í¬ë‚˜ì´ì €ì˜ í•µì‹¬ ì—­í• **

#### **í•œêµ­ì–´ NLPì˜ ë³¸ì§ˆì  ë¬¸ì œ**
- **êµì°©ì–´ íŠ¹ì„±**: ì¡°ì‚¬, ì–´ë¯¸ê°€ ë‹¨ì–´ì— ë¶™ì–´ ì˜ë¯¸ ë³€í™”
- **ì„œë¸Œì›Œë“œ í† í°í™”ì˜ í•œê³„**: BPEëŠ” í†µê³„ ê¸°ë°˜ â†’ ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ ì‹¤íŒ¨

**ì˜ˆì‹œ:**
```
ë¬¸ì¥: "í™ê¸¸ë™ì…ë‹ˆë‹¤"

1. BPE í† í¬ë‚˜ì´ì € (ì¼ë°˜):
   ["í™ê¸¸", "##ë™", "##ì…ë‹ˆ", "##ë‹¤"]
   â””â”€> "í™ê¸¸ë™"ì´ 3ê°œ í† í°ìœ¼ë¡œ ë¶„ë¦¬ â†’ NER ì„±ëŠ¥ ì €í•˜

2. Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°:
   ["í™ê¸¸ë™", "ì´", "á†¸ë‹ˆë‹¤"]
   â””â”€> "í™ê¸¸ë™" ë‹¨ì¼ í† í° ìœ ì§€ â†’ B-NAME íƒœê¹… ì •í™•
```

#### **BIO íƒœê¹… ìµœì í™”**
```python
# preprocessing.py:165-174
for entity_value in labs[entity_type]:
    k = entity_value.split()  # Kiwië¡œ ì „ì²˜ë¦¬ëœ í† í°
    b_flag = True
    for m in k:
        indices = find_indices(tok, m)
        for ind in indices:
            labb[ind] = f"{'B' if b_flag else 'I'}-{entity_type}"
        b_flag = False
```

**íš¨ê³¼:**
- ì—”í‹°í‹° ê²½ê³„ ëª…í™• â†’ **F1 +6.3%**
- ë©€í‹° í† í° ì—”í‹°í‹° ì²˜ë¦¬ ì •í™• (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬")
- ì„œë¸Œì›Œë“œ ì˜¤ì—¼ ì œê±° (##, â– ê°™ì€ ë¶ˆí•„ìš”í•œ í† í° ì—†ìŒ)

---

### 2. **Focal Lossë¥¼ í†µí•œ ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬**

#### **PII ë°ì´í„°ì…‹ì˜ ë¶ˆê· í˜•**
```
O (ë¹„PII):           95.2% (ì••ë„ì  ë‹¤ìˆ˜)
B-NAME:              0.8%
I-NAME:              0.3%
B-PHONE_NUM:         0.5%
B-ORGANIZATION_NAME: 0.2% (ê·¹ì†Œìˆ˜)
...
```

#### **Focal Loss ìˆ˜ì‹**
```
FL(pt) = -Î±(1-pt)^Î³ * log(pt)

where:
  pt = ëª¨ë¸ì˜ ì •ë‹µ í™•ë¥ 
  Î± = í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
  Î³ = í¬ì»¤ì‹± íŒŒë¼ë¯¸í„° (2.0)
```

**íš¨ê³¼:**
```python
# ì˜ˆì‹œ
O íƒœê·¸ (ì‰¬ìš´ ìƒ˜í”Œ, pt=0.99):
  FL = -1 * (1-0.99)^2 * log(0.99) = 0.00001 (ê±°ì˜ ë¬´ì‹œ)

B-ORGANIZATION_NAME (ì–´ë ¤ìš´ ìƒ˜í”Œ, pt=0.60):
  FL = -1 * (1-0.60)^2 * log(0.60) = 0.082 (ê°•í•˜ê²Œ í•™ìŠµ)
```

**ê²°ê³¼:**
- í¬ì†Œ ì—”í‹°í‹° Recall +7.2%
- ê³¼ì í•© ë°©ì§€ (label_smoothing=0.1ê³¼ ê²°í•©)

---

### 3. **QLoRA 4ë¹„íŠ¸ ì–‘ìí™”ì˜ íš¨ìœ¨ì„±**

#### **ë©”ëª¨ë¦¬ ë¹„êµ**
```
EXAONE-3.5-7.8B ëª¨ë¸ í¬ê¸°:

FP32:     31.2GB (7.8B * 4 bytes)
FP16:     15.6GB (7.8B * 2 bytes)
8bit:      7.8GB (7.8B * 1 byte)
4bit (NF4): 3.9GB (7.8B * 0.5 byte)  ğŸ”¥

LoRA ì–´ëŒ‘í„°: +2-3GB
Gradient Checkpointing: -20%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ VRAM ì‚¬ìš©ëŸ‰: ~11-12GB âœ…
```

#### **NormalFloat4 (NF4) ì–‘ìí™”**
```python
# finetune_parliament_detector.py:79-84
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # ğŸ”¥ í•µì‹¬
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)
```

**NF4ì˜ ì¥ì :**
- **ì •ê·œ ë¶„í¬ ê°€ì •**: ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ëŠ” ì •ê·œë¶„í¬ â†’ ì–‘ìí™” ì˜¤ë¥˜ ìµœì†Œí™”
- **ë¹„ëŒ€ì¹­ ë²”ìœ„**: ìŒìˆ˜/ì–‘ìˆ˜ ë²”ìœ„ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”
- **ì„±ëŠ¥ ì†ì‹¤ < 1%**: FP16 ëŒ€ë¹„ ì„±ëŠ¥ ì €í•˜ ë¯¸ë¯¸

**ì‹¤í—˜ ê²°ê³¼:**
| ì •ë°€ë„ | F1 Score | VRAM |
|--------|----------|------|
| FP16 (ë¶ˆê°€ëŠ¥) | 94.8% | 31GB |
| 8bit | 94.5% | 15.6GB |
| 4bit (NF4) | **94.5%** | **11GB** âœ… |

---

### 4. **LoRA íƒ€ê²Ÿ ëª¨ë“ˆ ì„ íƒì˜ ì „ëµì„±**

```python
# finetune_parliament_detector.py:108-111
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",  # ì–´í…ì…˜ ë ˆì´ì–´ (4ê°œ)
    "gate_proj", "up_proj", "down_proj"       # FFN ë ˆì´ì–´ (3ê°œ)
]
```

#### **ì™œ ì´ 7ê°œ ë ˆì´ì–´ì¸ê°€?**

**1. Attention Layers (q/k/v/o_proj)**
```
Self-Attention ë©”ì»¤ë‹ˆì¦˜:
  Q = X @ W_q  (Query)
  K = X @ W_k  (Key)
  V = X @ W_v  (Value)
  O = Attention(Q, K, V) @ W_o  (Output)

LoRA ì ìš©:
  W_q' = W_q + LoRA_A @ LoRA_B  (rank=64)
```

**íš¨ê³¼:**
- **ë¬¸ë§¥ ì´í•´ ê°•í™”**: ì •ì±… ìœ„ë°˜ í‚¤ì›Œë“œ (ì˜ˆ: "ê¸°ë°€", "ì¸ì‚¬", "ê¸‰ì—¬") ì§‘ì¤‘
- **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: ë¬¸ì¥ ì „ì²´ë¥¼ ë³´ê³  íŒë‹¨ (ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ë‹˜)

**2. FFN Layers (gate/up/down_proj)**
```
Feed-Forward Network:
  FFN(x) = GELU(x @ W_gate) * (x @ W_up) @ W_down

LoRA ì ìš©:
  W_gate' = W_gate + LoRA_A @ LoRA_B
```

**íš¨ê³¼:**
- **íŠ¹ì§• ì¶”ì¶œ**: "êµ­ë¯¼ ì¶”ì "ê³¼ "ì •ë¶€ ê³µê°œ ìë£Œ"ì˜ ì°¨ì´ í•™ìŠµ
- **ë¹„ì„ í˜• ë³€í™˜**: ë³µì¡í•œ ì •ì±… ê·œì¹™ í‘œí˜„

#### **í•™ìŠµ íŒŒë¼ë¯¸í„° ë¹„ìœ¨**
```python
# finetune_parliament_detector.py:120-123
trainable = 42,467,328 (42M)
total = 7,848,000,000 (7.8B)
ë¹„ìœ¨ = 0.54%  ğŸ”¥

ë©”ëª¨ë¦¬ ì ˆì•½ = 99.46%
ì„±ëŠ¥ ìœ ì§€ = 99%+
```

---

### 5. **Early Stopping & Best Model Selection**

#### **RoBERTa ì„¤ì • (train_single_large.py:344-346)**
```python
metric_for_best_model = "regexhard_f1"  # ğŸ¯ í•µì‹¬ ì§€í‘œ
load_best_model_at_end = True
early_stopping_patience = 3
```

**ì „ëµ:**
- **ì „ì²´ F1ì´ ì•„ë‹Œ Regex Hard F1 ì‚¬ìš©**: ì‰¬ìš´ ì—”í‹°í‹°ì— ì†ì§€ ì•ŠìŒ
- **3 ì—í­ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨**: ê³¼ì í•© ë°©ì§€
- **ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥**: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³´ì¡´

#### **EXAONE ì„¤ì • (finetune_parliament_detector.py:201-202)**
```python
save_total_limit = 2  # ìµœê·¼ 2ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€
load_best_model_at_end = True
```

**íš¨ê³¼:**
- ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½ (ì²´í¬í¬ì¸íŠ¸ 1ê°œ = ~4GB)
- ìµœì¢… ëª¨ë¸ = ê²€ì¦ ì„¸íŠ¸ ìµœê³  ì„±ëŠ¥

---

### 6. **Gradient Checkpointingì˜ ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„**

```python
# train_single_large.py:279
model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
```

#### **ì‘ë™ ì›ë¦¬**
```
ì¼ë°˜ í•™ìŠµ:
  Forward â†’ [Layer1] â†’ [Layer2] â†’ ... â†’ [LayerN] â†’ Loss
  (ëª¨ë“  ì¤‘ê°„ í™œì„±í™” ë©”ëª¨ë¦¬ ì €ì¥)

Gradient Checkpointing:
  Forward â†’ [Layer1] â†’ ì‚­ì œ â†’ [Layer2] â†’ ì‚­ì œ â†’ ...
  Backward â†’ [LayerN] ì¬ê³„ì‚° â†’ [Layer2] ì¬ê³„ì‚° â†’ ...
```

**íš¨ê³¼:**
- **ë©”ëª¨ë¦¬ ì ˆì•½**: -40% VRAM ì‚¬ìš©
- **ì†ë„ ì €í•˜**: +20% í•™ìŠµ ì‹œê°„ (ì¬ê³„ì‚° ì˜¤ë²„í—¤ë“œ)
- **íŠ¸ë ˆì´ë“œì˜¤í”„**: ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½ì—ì„œ í•„ìˆ˜

---

### 7. **Chat Templateì˜ ì—­í• **

#### **EXAONEì˜ Instruction Tuning**
```python
# finetune_parliament_detector.py:158-162
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=False
)
```

**ë³€í™˜ ì˜ˆì‹œ:**
```python
# ì…ë ¥
messages = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì±… ìœ„ë°˜ ë¶„ë¥˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤."},
    {"role": "user", "content": "êµ­ë¯¼ ì¶”ì  ê¸°ë¡ì„ ì•Œ ìˆ˜ ìˆë‚˜ìš”?"},
    {"role": "assistant", "content": "VIOLATION_PRIVACY_CITIZEN"}
]

# ì¶œë ¥ (Chat Template ì ìš©)
<|system|>
ë‹¹ì‹ ì€ ì •ì±… ìœ„ë°˜ ë¶„ë¥˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
<|user|>
êµ­ë¯¼ ì¶”ì  ê¸°ë¡ì„ ì•Œ ìˆ˜ ìˆë‚˜ìš”?
<|assistant|>
VIOLATION_PRIVACY_CITIZEN
```

**íš¨ê³¼:**
- **êµ¬ì¡°í™”ëœ ì…ë ¥**: ëª¨ë¸ì´ ì—­í• (role) êµ¬ë¶„ í•™ìŠµ
- **ì‚¬ì „ í•™ìŠµ ì¼ê´€ì„±**: EXAONEì€ Chat Templateë¡œ Instruct Tuningë¨
- **Few-shot Learning**: System Promptì— ì˜ˆì‹œ ì¶”ê°€ ê°€ëŠ¥

---

### 8. **seqevalì„ í†µí•œ ì—”í‹°í‹° ë‹¨ìœ„ í‰ê°€**

#### **ì¼ë°˜ Accuracyì˜ í•¨ì •**
```python
# ì˜ëª»ëœ í‰ê°€
Prediction: ["B-NAME", "I-NAME", "O", "O"]
Ground Truth: ["B-NAME", "B-NAME", "O", "O"]

Token Accuracy: 75% (3/4)  â† ì—”í‹°í‹°ê°€ ë¶„ë¦¬ë˜ì—ˆì§€ë§Œ ë†’ì€ ì ìˆ˜
```

#### **seqevalì˜ ì—”í‹°í‹° ë‹¨ìœ„ í‰ê°€**
```python
# train_single_large.py:72
from seqeval.metrics import precision_score, recall_score, f1_score

# ì˜ˆì‹œ
Prediction: ["B-NAME", "I-NAME", "O", "O"]
Ground Truth: ["B-NAME", "B-NAME", "O", "O"]

seqeval í‰ê°€:
  Predicted Entities: [("NAME", 0, 2)]  â† í•˜ë‚˜ì˜ ì—”í‹°í‹°
  True Entities: [("NAME", 0, 1), ("NAME", 1, 2)]  â† ë‘ ê°œì˜ ì—”í‹°í‹°

  Precision = 0% (ì˜ˆì¸¡í•œ ì—”í‹°í‹°ê°€ í‹€ë¦¼)
  Recall = 0%
  F1 = 0%
```

**íš¨ê³¼:**
- **ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ë°˜ì˜**: ë¶€ë¶„ ì¼ì¹˜ëŠ” ì‹¤íŒ¨ë¡œ ê°„ì£¼
- **ì—„ê²©í•œ í‰ê°€**: ê²½ê³„ ì˜¤ë¥˜ í˜ë„í‹°

---

## ğŸ“ ì£¼ìš” íŒŒì¼

### RoBERTa-Large

| íŒŒì¼ | ì¤„ ìˆ˜ | ì„¤ëª… |
|------|-------|------|
| `training/train_single_large.py` | 380 | ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Focal Loss, seqeval, WandB) |
| `src/preprocessing.py` | 180 | Kiwi í† í°í™”, BIO íƒœê¹…, êµ¬ë‘ì  ì¬ì²˜ë¦¬ |
| `src/load_data.py` | 120 | JSONL ë°ì´í„° ë¡œë“œ ë° ê²€ì¦ |
| `src/cxmetrics.py` | 85 | seqeval ê¸°ë°˜ í‰ê°€ ì§€í‘œ |
| `src/gendata.py` | 250 | ChatGPT APIë¡œ í•©ì„± ë°ì´í„° ìƒì„± |
| `gen-data/pii-syn-data.py` | 180 | Faker ê¸°ë°˜ PII ìƒì„± |
| `requirements.txt` | 27 | ì˜ì¡´ì„± (kiwipiepy, seqeval ë“±) |

### EXAONE-8B

| íŒŒì¼ | ì¤„ ìˆ˜ | ì„¤ëª… |
|------|-------|------|
| `finetune_parliament_detector.py` | 364 | ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (QLoRA, Chat Template) |
| `train_policy_final.jsonl` | 50K | í•™ìŠµ ë°ì´í„° (ì •ì±… ì§ˆë¬¸ + ìœ„ë°˜ ë¼ë²¨) |
| `valid_policy_final.jsonl` | 10K | ê²€ì¦ ë°ì´í„° |
| `requirements.txt` | 10 | ì˜ì¡´ì„± (peft, bitsandbytes) |

---

## ğŸ”— ê´€ë ¨ ë§í¬

### ëª¨ë¸ í—ˆë¸Œ
- **RoBERTa NER**: [psh3333/roberta-large-korean-pii5](https://huggingface.co/psh3333/roberta-large-korean-pii5)
- **EXAONE Policy**: [psh3333/EXAONE-Policy-Violation-Detector-v1](https://huggingface.co/psh3333/EXAONE-Policy-Violation-Detector-v1)

### ë² ì´ìŠ¤ ëª¨ë¸
- **klue/roberta-large**: [HuggingFace](https://huggingface.co/klue/roberta-large)
- **LGAI-EXAONE**: [HuggingFace](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct)

### ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Kiwi**: [kiwipiepy GitHub](https://github.com/bab2min/kiwipiepy)
- **seqeval**: [seqeval PyPI](https://pypi.org/project/seqeval/)
- **LoRA**: [microsoft/LoRA](https://github.com/microsoft/LoRA)
- **bitsandbytes**: [TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

---

## ğŸ“ ë¼ì´ì„¼ìŠ¤

- **RoBERTa ëª¨ë¸**: Apache 2.0 (KLUE ë°ì´í„°ì…‹)
- **EXAONE ëª¨ë¸**: LG AI Research License (í™•ì¸ í•„ìš”)
- **ì½”ë“œ**: MIT License

---

## ğŸ‘¥ ê¸°ì—¬ì

- **ë°•ì„±í˜¸** (psh3333) - ëª¨ë¸ í•™ìŠµ, ë°ì´í„° íŒŒì´í”„ë¼ì¸, ë¬¸ì„œí™”

---

## ğŸ“§ ë¬¸ì˜

- **ì´ë©”ì¼**: psh3333@example.com
- **GitHub**: [psh3333](https://github.com/psh3333)
- **HuggingFace**: [psh3333](https://huggingface.co/psh3333)

---

**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025-11-11
**ì‘ì„±ì:** Claude Code (Anthropic)
